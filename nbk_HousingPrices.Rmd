---
title: "Kaggle Housing Prices"
author: "Zeno Lee"
abstract: |
  The following notebook is designed to simply explore and learn practical machine learning techniques utilizing data retrieved from Kaggle.  
output: 
  html_notebook:
    toc: true
    toc_depth: 2
---

#Links

https://www.kaggle.com/aniruddhachakraborty/lasso-gbm-xgboost-top-20-0-12039-using-r

https://www.kaggle.com/couyang/cannabis-interactive-eda-drilldown-plus-nlp

https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4

https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard

https://www.kaggle.com/erikbruin/house-prices-lasso-xgboost-and-a-detailed-eda

https://www.kaggle.com/bwboerman/r-data-table-glmnet-xgboost-with-caret

*Neural network example*     
https://www.hackerearth.com/practice/machine-learning/machine-learning-projects/python-project/tutorial/

# Initialization

Set Libraries
```{r libraries, eval=TRUE, message=FALSE, warning=FALSE}
# Use R 3.4, some packages are not available for R 3.5

library(data.table)# Data tables 
library(plotly)    # Plots and Graphs
library(Boruta)    # Runs a Boruta Feature Analysis
library(dplyr)     # Data Manipulation
library(corrplot)  # Correlation Plots
library(xgboost)   # Runs Extreme Gradient Boosting
library(Matrix)    # For sparse matrices
library(shiny)     # For Layouts
library(mlr)       # Machine Learning
#library(tidyr)     # Data Manipulation
library(skimr)     # Summary Statistics
#library(DMwR)      # KNN Data Imputation
library(mice)      # Multiple Imputation
library(VIM)       # Visualizations
library(caret)
library(glmnet)
library(highcharter)
library(gridExtra) 
library(corrplot)

# For Skewness
library(MASS)      # Box Cox Transformation
library(moments)

#install.packages("mmpf",lib="C:/Zeno/R/libraries/")
#install.packages("mmpf",lib="C:/Program Files/R/R-3.4.3/library")
```


```{r}
install.packages("data.table")# Data tables 
install.packages("plotly")    # Plots and Graphs
install.packages("Boruta")    # Runs a Boruta Feature Analysis
install.packages("dplyr")     # Data Manipulation
install.packages("corrplot")  # Correlation Plots
install.packages("xgboost")   # Runs Extreme Gradient Boosting
install.packages("Matrix")    # For sparse matrices
install.packages("shiny")     # For Layouts
install.packages("mlr")       # Machine Learning
install.packages("tidyr")     # Data Manipulation
install.packages("skimr")     # Summary Statistics
install.packages("DMwR")      # KNN Data Imputation
install.packages("mice")      # Multiple Imputation
install.packages("VIM")       # Visualizations
install.packages("caret")
install.packages("glmnet")
install.packages("highcharter")
install.packages("gridExtra") 
install.packages("corrplot")

# For Skewness
install.packages("MASS")      # Box Cox Transformation
install.packages("moments")

```


# Import Data
```{r Data, eval=TRUE, warning=FALSE, message=FALSE}
dir.data <- "E:/Zeno/R/kggl_house_price/Data/"
data.test <- fread(paste(dir.data,"test.csv",sep=""),stringsAsFactors = FALSE)
data.train <- fread(paste(dir.data,"train.csv",sep=""), stringsAsFactors = FALSE)
```

# Linear Regression Background

Linear Regression has 5 key assumptions    
    1.  Linear Relationship    
    2.  Multivariate Normality    
    3.  Non-collinearity    
    4.  No Auto-Correlation    
    5.  Homoscedasticity    

# EDA - Exploratory Data Analysis

## Data Summary

The base method of quickly profile the data.

```{r}
summary(data.train)
```

I want to use skimr package to do a quick profile on the dataset.

```{r}
#data.train %>% skimr::skim() %>% kable()
data.train %>% skim()

```

Just a quick sanity check on the response variable

```{r}
data.train %>% dplyr::group_by(OverallQual) %>% summarize(SalePrice=mean(SalePrice)) %>% plot_ly(x=~OverallQual, y=~SalePrice, type="bar")
```

This isn't necessarily a place for this particular histogram.  I simply put it here for my own reference on building drill down charts through highcharter.

```{r}

# High Chart driven by column names.  Name, y, and drilldown

df <- data.train %>% 
  dplyr::group_by(name=TotRmsAbvGrd,drilldown=TotRmsAbvGrd) %>% 
  summarize(y=n())

df_drill <- data.train %>% 
  dplyr::group_by(TotRmsAbvGrd, FullBath) %>%
  dplyr::summarize(y=n()) %>%
  dplyr::group_by(name=TotRmsAbvGrd,id=TotRmsAbvGrd) %>%
  do(data=list_parse(
    mutate(.,name=FullBath, drilldown=paste(TotRmsAbvGrd, FullBath, sep=":")) %>%
      group_by(name,drilldown) %>%
      summarise(y=sum(y)) %>%
      dplyr::select(name, y, drilldown)
  ))

h <- highchart() %>% 
  hc_chart(type="column") %>%
  hc_xAxis(type="category") %>%
  hc_add_series(name="Count", data=df, colorByPoint=1) %>%
  hc_drilldown(
    allowPointDrilldown=TRUE,
    series=list_parse(df_drill)
  ) %>%
  hc_legend(enabled=F) %>%
  hc_add_theme(hc_theme_darkunica()) %>%
  hc_title(text="Count by Rooms and Baths")

#High Charts wont appear in a notebook as is, so I need to embed it in a Shiny Page

fluidPage(
  fluidRow(
    h
  )
)

```


## Outlier Analysis

For this outlier analysis, we know that intuitively that General Living Area (Square Footage) is a significant variable when predicting housing prices.  i.e. When all else is equal, the bigger the home the more it's going to cost.  So, I want to see how the data looks relative to the General Living Area.

## General Living Area

```{r}
plot_ly(type="scatter",mode="markers"
        ,x=data.train$GrLivArea
        ,y=data.train$SalePrice) %>%
  layout(title="Gen. Living Area vs Sale Price")

```

```{r Rmv_Outliers, eval=TRUE}
anchor.train <- data.train %>% filter(GrLivArea < 4000)

plot_ly(anchor.train
        ,mode="markers"
        ,type="scatter"
        ,marker=list(color="purple")
        ,x=~GrLivArea
        ,y=~SalePrice) %>%
  layout(title="Gen. Living Area vs Sale Price without Outliers")

```

Box Whisker Plots

```{r}
highchart() %>% 
  hc_add_series_boxplot(x=anchor.train$SalePrice,
                        by=anchor.train$MSZoning
                        ,name="MS Zone") %>%
  hc_add_theme(hc_theme_flatdark())

```

## Sale Price Skewness

Sale Price is the target/response variable, so let's do a little analysis on it.  Let's get a density plot of the response variable (basically frequency for continuous variables).

```{r Density_Plot, eval=TRUE}
h1 <- hchart(density(anchor.train$SalePrice)
             ,type="area"
             ,color="#ff9999"
             ,name="Sale Price") %>%
  hc_title(text="Density Plot of Sale Price")
h2 <- hchart(density(log(anchor.train$SalePrice))
             ,type="area"
             ,color="#80aaff"
             ,name="Log of Sale Price") %>%
  hc_title(text="Density Plot of Log Sale Price")

fluidPage(
  fluidRow(column(6,h1)
           ,column(6,h2))
)
```

We can see that Sale Price is skewed, where as the log transform of the Sale Price looks to be more normally distributed.

```{r QQ_Plot, eval=TRUE}
p1 <- ggplot(anchor.train, aes(sample=SalePrice)) + 
  stat_qq(colour="red") + 
  stat_qq_line(colour="blue",size=1.5) + 
  ggtitle("Normal Q-Q Plot: Sale Price")

p2 <- ggplot(anchor.train, aes(sample=log(SalePrice))) + 
  stat_qq(colour="red") + 
  stat_qq_line(colour="blue",size=1.5) + 
  ggtitle("Normal Q-Q Plot: Log of Sale Price")

grid.arrange(p1, p2, ncol=2)
```

Looking at the QQ Plot, we can confirm that the transform is definitely more normally distributed.


# Data Processing

## Missing Values?

% of data that is NULL/NA by column
```{r}
#sapply(data.train,function(x) sum(x=="NULL"))  # Use when Null
round(sapply(anchor.train,function(x) sum(is.na(x))) / nrow(anchor.train) * 100,digits=3)   # Use when NA
```


```{r Combine, eval=TRUE, message=FALSE, warning=FALSE}
# We remove SalePrice because we don't want the One Hot Encoding to consider it as a feature
response.train <- log(anchor.train$SalePrice)

#Copy the Train Data and separate out the response variable
anchor.train <- anchor.train %>% dplyr::select(-SalePrice) 

# Create a combined data set where we can do the transformations all at once
anchor <- rbind(anchor.train, data.test)
```

>**Why Combine Train & Test?**  <br>1.  We can be consistent on how we transform the data.  <br>2.  When using xgboost, the same number of columns must be available for the prediction or else you will get incorrect results.  If train has a feature level not available in test, then One-Hot Encoding will not produce the appropriate columns.

Now we need to separate out the Numeric vs Categorical variables.  We do need to understand the variables as MBSubClass looks like a numerical variable, but closer inspection shows that the integer values simply represent a housing type.  We do not want the Machine Learning model to assume there is an inherent ranking with those values.  We'll need to include them in the One-Hot Encoding.


```{r Feature_Types, eval=TRUE}
anchor <- anchor %>% mutate(MSSubClass=as.character(MSSubClass))

features <- sapply(anchor,typeof)
features.category <- names(features[features == "character"])
features.numeric <- names(features[features != "character"])

```

## Imputation of Data

Imputing data is simply the process of replacing missing data.  There are various ways to do it for both Categorical and Numeric/Continuous variables.  Some examples for categorical variables is to bin/group them with another factor within the independent variable.  If the distribution is high enough, it is sometimes appropriate to give missing it's own category.  For continuous variables, sometimes the easiest is to replace the missing data with the mean/median/mode of the variable.  Some more advanced techniques would be to use KNN (k-Nearest Neighbor) Imputation.  Note that a more advanced technique doesn't mean it will yield better results.

### Categorical Data {.tabset}

We want to focus on the features/independent variables that have anything in the missing column.

```{r}
anchor[features.category] %>% skim() 
```


#### Utilities

We can see that other than 3 records, everything is All Public Utilities. Basically this field is useless for modeling as it has absolutely no variance.  

```{r}
anchor %>% group_by(Utilities) %>% tally()
```

```{r}
anchor <- anchor %>% dplyr::select(-Utilities)
```

#### SaleType

There is only 1 missing record and will have no impact on the analysis.  We will bucket it with the "standard" saletype, which will be the WD category.

```{r}
anchor %>% group_by(SaleType) %>% tally()
```

```{r}
anchor$SaleType[is.na(anchor$SaleType)] <- "WD"
```


#### KitchenQual

There is only a single missing value for kitchen quality.  Let's put that in the "typical/average" category.

```{r}
anchor %>% group_by(KitchenQual) %>% tally()
```

```{r}
anchor$KitchenQual[is.na(anchor$KitchenQual)] <- "TA"
```


#### MSZoning

Ideally we could cluster the missing feature based on the other features to better classify what type of home this is.  But since we are talking about only 4 missing values, we will bucket it with the "RL" or "Residential Low Density" group.

```{r}
anchor %>% group_by(MSZoning, MSSubClass) %>% tally()
```


```{r}
anchor$MSZoning[is.na(anchor$MSZoning)] <- "RL"
```

#### Functional

```{r}
anchor %>% group_by(Functional) %>% tally()
```

```{r}
anchor$Functional[is.na(anchor$Functional)] <- "Typ"
```


#### Electrical

```{r}

anchor %>% group_by(Electrical) %>% tally()

```

```{r}
anchor$Electrical[is.na(anchor$Electrical)] <- "SBrkr"
```

#### Exterior1st & Exterior2nd

```{r}
anchor %>% group_by(Exterior1st, Exterior2nd) %>% tally() %>% arrange(desc(n))
```


```{r}
anchor$Exterior1st[is.na(anchor$Exterior1st)] = "VinylSd"
anchor$Exterior2nd[is.na(anchor$Exterior2nd)] = "VinylSd"

```


#### None Type

The are a group of variables, where when the values are missing, it is actually "None".  We will replace only those variables with when missing.

```{r}
for (x in c("Alley", "PoolQC", "MiscFeature", "Fence", "FireplaceQu", "GarageType", 
    "GarageFinish", "GarageQual", "GarageCond", "BsmtQual", "BsmtCond", 
    "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "MasVnrType")) {
    anchor[is.na(anchor[, x]), x] <- "None"
}
```

### Check Categorial Variables

```{r}
features <- sapply(anchor,typeof)
features.category <- names(features[features == "character"])
features.numeric <- names(features[features != "character"])

anchor[features.category] %>% skim()
```

We see that we no longer have any missing values for any of the categorial variables.

## YrSold &  MoSold

We want to remove the inherent ranking of the Year and Month sold by changing them from integer to character values.  This may not necessarily the right approach.  Year/Month is essentially a sequential order  However, I'm assuming that people do not consider the time of year in which they buy a home.

```{r}
anchor$YrSold <- as.character(anchor$YrSold)
anchor$MoSold <- as.character(anchor$MoSold)

anchor %>% group_by(YrSold, MoSold) %>% summarize(Count=n()) 
```

## Imputation of Numerical Data

```{r}
features <- sapply(anchor,typeof)
features.category <- names(features[features == "character"])
features.numeric <- names(features[features != "character"])

anchor[features.numeric] %>% skim()
```

### Garage Year Built

```{r}
anchor %>% group_by(GarageType) %>% tally()

```

```{r}
anchor %>% group_by(YearBuilt, YearRemodAdd, GarageYrBlt) %>% 
  filter(is.na(GarageYrBlt)==TRUE | GarageYrBlt >2010 ) %>% 
  tally()
```

There is 1 instance where the Garage Year built is 2207, which is definitely an error.  For that one example, we will set it to the year of the remodel as it appears that is what it was supposed to be.

```{r}
anchor$GarageYrBlt[anchor$GarageYrBlt == 2207] <- 2007

anchor %>% group_by(GarageYrBlt) %>% tally()

```

For any numeric variable, let's replace anything missing with 0.  For the majority of the variables, if not all of them, the documentation stated that when missing, it usually means 0.  

```{r}

for (x in features.numeric){
  anchor[is.na(anchor[,x]),x] <- 0
}

```

# Correlation Analysis

Though dependent on the model/ML Method one uses, highly correlated variables could completely invalidate a model.  Understanding correlations between variables as part of the exploratory data analysis phase as it will dictate whether features need to be censored from training of models.

Ordinal:  Categorical variable with a rank order
Nominal:  Categorical variable with no ranking
Continuous: Numeric variable

Correlation calculations are differentiated between categorical and numerical variables.  There are appropriate tests based on the type of data it is:  
    *  Ordinal vs Ordinal:  Pearson?
    *  Nominal vs Nominal:  Chi-Square?
    *  Ordinal vs Nominal:  
    *  Continuous vs Nominal: ANOVA, Pearson Product Moment
    *  Continuous vs Ordinal:  

I'm still trying to understand the pros/cons of correlation tests.

>*Note: The response/target variable is not in the data set*

```{r}
corrplot(cor(anchor[1:1456,features.numeric]),title = "Correlation Plot", method = "square")

```

The above plot is good, but it's static and it's difficult to see.  I've decided to reproduce the correlation plot in something more interactive.

A correlation plot is basically a heatmap or treemap with even squares with shaded colors.  I found a function on google where someone created a "heatmap" to create a correlation plot.

```{r}

hchart.cor <- function(object, ...) {
  
  df <- as.data.frame(object)
  is.num <- sapply(df, is.numeric)
  df[is.num] <- lapply(df[is.num], round, 2)
  dist <- NULL
  
  x <- y <- names(df)
  
  df <- tbl_df(cbind(x = y, df)) %>% 
    gather(y, dist, -x) %>% 
    mutate(x = as.character(x),
           y = as.character(y)) %>% 
    left_join(data_frame(x = y,
                         xid = seq(length(y)) - 1), by = "x") %>% 
    left_join(data_frame(y = y,
                         yid = seq(length(y)) - 1), by = "y")
  
  ds <- df %>% 
    select_("xid", "yid", "dist") %>% 
    list.parse2()
  
  fntltp <- JS("function(){
                  return this.series.xAxis.categories[this.point.x] + ' ~ ' +
                         this.series.yAxis.categories[this.point.y] + ': <b>' +
                         Highcharts.numberFormat(this.point.value, 2)+'</b>';
               ; }")
  cor_colr <- list( list(0, '#FF5733'),
                    list(0.5, '#F8F5F5'),
                    list(1, '#2E86C1')
  )
  highchart() %>% 
    hc_chart(type = "heatmap") %>% 
    hc_xAxis(categories = y, title = NULL) %>% 
    hc_yAxis(categories = y, title = NULL) %>% 
    hc_add_series(data = ds) %>% 
    hc_plotOptions(
           series = list(
             boderWidth = 0,
             dataLabels = list(enabled = TRUE)
    )) %>% 
    hc_tooltip(formatter = fntltp) %>% 
    hc_legend(align = "right", layout = "vertical",
              margin = 0, verticalAlign = "top",
              y = 25, symbolHeight = 280) %>% 
    hc_colorAxis(  stops= cor_colr,min=-1,max=1)
}

hchart.cor(cor(anchor[1:1456,features.numeric]))

```

Obviously, there are too many variables that this plot is difficult to read.  Since we aren't concerned with variables that aren't correlated, we'd want to filter on the variables that ARE correlated so we can further analyze the relationship.

# Feature Engineering {.tabset}

## BsmtFinSF1 & BsmtFinSF2

Based on the definition, it appears that if a basement has multiple finishes, it uses a 2nd variable to track the square footage of the next finish.  There may be some opportunity here for some feature engineering.  Combining SF1 and SF2.  Maybe keeping it separated.  TBD.

```{r}
anchor %>% dplyr::select(BsmtFinSF1, BsmtFinSF2, TotalBsmtSF)
```

## Neighborhood

Binning some of these neighborhoods together 

```{r}
df <- data.train %>% 
  group_by(Neighborhood) %>% 
  summarize(Med_SalePrice=median(SalePrice)
            ,N_Count=n()) %>% 
  arrange(Med_SalePrice) 

df$Neighborhood <- factor(df$Neighborhood,levels=df$Neighborhood)

plot_ly() %>% add_trace(type="bar"
        ,x=df$Neighborhood
        ,y=df$Med_SalePrice
        ,name="Median Price"
        ,marker=list(color="#ffaa80"
                     ,line=list(color="#ff7733",width=1.25))) %>%
  add_trace(type="scatter"
            ,mode="lines"
            ,x=df$Neighborhood
            ,y=df$N_Count
            ,name="Counts"
            ,marker=list(color="#3333ff")
            ,line=list(color="#6666ff")
            ,yaxis="y2") %>%
  layout(title="Neighborhoods by Median Sale Price and Counts"
         ,yaxis=list(side="left",title="Median Sale",zeroline=FALSE,showgrid=FALSE)
         ,yaxis2=list(side="right",title="Count",overlaying="y",showgrid=FALSE, zeroline=FALSE) )

```


### Skewed Features

```{r}

features <- sapply(anchor,typeof)
features.category <- names(features[features == "character"])
features.numeric <- names(features[features != "character"])
features.numeric <- features.numeric[features.numeric != "Id"]

skewed_features <- sapply(features.numeric,function(x) {
  skewness(anchor[[x]],na.rm=TRUE)
})

# Filter for features with skewness > .75
skewed_features <- skewed_features[abs(skewed_features) > .75]

names(skewed_features)
```

```{r}
h.grliv <- hchart(density(anchor$GrLivArea)
                  ,type="area"
                  ,color="#ff9999"
                  ,name="General Living Area")

```


```{r}
# Perform a Box-Cox transformation on the skewed variables
for (x in names(skewed_features)) {
  bc <- BoxCoxTrans(anchor[[x]], lambda=.15)
  anchor[[x]] <- predict(bc, anchor[[x]])
  
}

h.bx.grliv <- hchart(density(anchor$GrLivArea)
                  ,type="area"
                  ,color="#80aaff"
                  ,name="Box Cox General Living Area")

fluidPage(
  fluidRow(column(6,h.grliv)
           ,column(6,h.bx.grliv))
)

```

```{r}
p2 <- ggplot(anchor, aes(sample=GrLivArea)) + 
  stat_qq(colour="red") + 
  stat_qq_line(colour="blue",size=1.5) + 
  ggtitle("Normal Q-Q Plot: Box Cox of General Living Area")

p2
```

## Inherent Rankings

Features about quality of a product has an inherent ranking to it.  Poor, satisfactory, good have a rank order that we want to preserve in the model.  

```{r}
cols = c("FireplaceQu", "BsmtQual", "BsmtCond", "GarageQual", "GarageCond", 
    "ExterQual", "ExterCond", "HeatingQC", "PoolQC", "KitchenQual", "BsmtFinType1", 
    "BsmtFinType2", "Functional", "Fence", "BsmtExposure", "GarageFinish", 
    "LandSlope", "LotShape", "PavedDrive", "Street", "Alley", "CentralAir", 
    "MSSubClass", "OverallCond", "YrSold", "MoSold")

FireplaceQu = c("None", "Po", "Fa", "TA", "Gd", "Ex")
BsmtQual = c("None", "Po", "Fa", "TA", "Gd", "Ex")
BsmtCond = c("None", "Po", "Fa", "TA", "Gd", "Ex")
GarageQual = c("None", "Po", "Fa", "TA", "Gd", "Ex")
GarageCond = c("None", "Po", "Fa", "TA", "Gd", "Ex")
ExterQual = c("Po", "Fa", "TA", "Gd", "Ex")
ExterCond = c("Po", "Fa", "TA", "Gd", "Ex")
HeatingQC = c("Po", "Fa", "TA", "Gd", "Ex")
PoolQC = c("None", "Fa", "TA", "Gd", "Ex")
KitchenQual = c("Po", "Fa", "TA", "Gd", "Ex")
BsmtFinType1 = c("None", "Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ")
BsmtFinType2 = c("None", "Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ")
Functional = c("Sal", "Sev", "Maj2", "Maj1", "Mod", "Min2", "Min1", "Typ")
Fence = c("None", "MnWw", "GdWo", "MnPrv", "GdPrv")
BsmtExposure = c("None", "No", "Mn", "Av", "Gd")
GarageFinish = c("None", "Unf", "RFn", "Fin")
LandSlope = c("Sev", "Mod", "Gtl")
LotShape = c("IR3", "IR2", "IR1", "Reg")
PavedDrive = c("N", "P", "Y")
Street = c("Pave", "Grvl")
Alley = c("None", "Pave", "Grvl")
MSSubClass = c("20", "30", "40", "45", "50", "60", "70", "75", "80", "85", 
    "90", "120", "150", "160", "180", "190")
OverallCond = NA
MoSold = NA
YrSold = NA
CentralAir = NA
levels = list(FireplaceQu, BsmtQual, BsmtCond, GarageQual, GarageCond, 
    ExterQual, ExterCond, HeatingQC, PoolQC, KitchenQual, BsmtFinType1, 
    BsmtFinType2, Functional, Fence, BsmtExposure, GarageFinish, LandSlope, 
    LotShape, PavedDrive, Street, Alley, CentralAir, MSSubClass, OverallCond, 
    YrSold, MoSold)

i = 1

for (c in cols) {
    if (c == "CentralAir" | c == "OverallCond" | c == "YrSold" | c == "MoSold") {
        anchor[, c] = as.numeric(factor(anchor[, c]))
    } else {
      anchor[, c] = as.numeric(factor(anchor[, c], levels = levels[[i]]))
    }
    i = i + 1
}

```

There are over 70 variables in the data set.  I want a more systematic approach to determine which variables I should be exploring deeper and including in the final model.

## Boruta Algorithm

**What is Boruta?**

It is a wrapper approach built around a random forest.  In a wrapper method, the classifier is used as a black box returning a feature ranking.  It is an ensemble method in which classification is performed by voting of multiple unbiased weak classifiers - decision trees.  These trees are independently developed on different bagging samples of the training set.

The importance measure of an attribute is obtained as the loss of accuracy classification caused by the random permutation of attribute values between objects.

**Boruta vs. Random Forest**

Boruta is supposed to be an improvement over Random Forest variable importance.  It uses randomization on top of results obtained from variable importance obtained from Random Forest to determine the truly important and statistically valid results.

**How does Boruta work?**

1.  Adds randomness to the given data set by creating shuffled copies of all features (which are called shadow features)
2.  Trains a random forest classifier on the extended data set and applies a feature importance measure (default is Mean Decrease Accuracy) to evaluate the importance of each feature where higher means more important
3.  At every iteration, it checks whether a real feature has a higher importance than the best of its shadow features (i.e. whether the feature has a higher Z score than the maximum Z score of its shadow features) and constantly removes features that are deemed highly unimportant
4.  Algorithm stops either when all features gets confirmed or rejected or it reaches a specified limit of random forest runs

**Applications in Insurance/Reinsurance?**

* It could validate model features that are being used.  
* Aid in developing models by subsetting explanatory variables so that a smaller subset is used instead of looking at insignificant variables.

Boruta can't handle Nulls, so we will replace them with "Missing" or "-1" depending on the field type.

```{r Clean_Nulls, eval=TRUE}
# I don't want to taint the original copy of the data
boruta.train <- anchor %>% dplyr::select(-Id)

for (x in features.category[features.category != "Id"])
{
  boruta.train[[x]][is.na(boruta.train[[x]])] <- "Missing"
}

for (x in features.numeric[features.numeric != "Id"])
{
  boruta.train[[x]][is.na(boruta.train[[x]])] <- -1
}

```


```{r Running_Boruta, eval=TRUE}
set.seed(123)

bor.results <- Boruta(boruta.train[1:1456,],response.train,maxRuns = 50,doTrace = 0)
```

Reviewing the Boruta Analysis Results

Summary of the Boruta Analysis
```{r Print Boruta Results, eval=TRUE}
print(bor.results)
```

Plot of the Boruta Analysis
```{r Plot Boruta Results, eval=TRUE}
plot(bor.results)
```

Let's review the variables

```{r Get Boruta Results, eval=TRUE}
results_df <- data.frame(attStats(bor.results))  #allStats is part of Boruta that summarizes the results into a table

results_df$feature <- factor(row.names(results_df),levels=unique(row.names(results_df))[order(results_df$meanImp,decreasing=FALSE)])

f <- list(
  family = "Courier New, monospace",
  size = 16,
  color = "#7f7f7f"
)

results_df <- results_df %>% arrange(desc(meanImp))

results_df

#write.csv(results_df, paste(dir.data, "Boruta.csv",sep=""))

```

Some of the "important" variables intuitively makes sense, i.e. General Living Area, Overall Quality, etc.  We will review the variables that seem to be of questionable decision.


```{r Plot_Features, eval=FALSE}
nfeat = 25

p <- results_df %>% 
  plot_ly(y=~head(feature,n=nfeat)
          ,x=~head(meanImp,n=nfeat)
          ,type="bar"
          ,orientation="h"
          ,name="Boruta"
          ,text=paste0("Decision: ",head(results_df$decision,nfeat))
          ) %>%
  layout(title="Boruta Feature Importance"
         ,xaxis=list(title="Mean Importance"
                     ,titlefont=f)
         ,yaxis=list(title="Feature"
                     ,titlefont=f)
         ,margin=list(l=150,r=50,b=50,t=50, pad=3))

fluidPage(
  fluidRow(p)
)

```


## Testing Automated Histograms

This was testing to see if I could create better histograms of the data.  

```{r Hist_Categorical, eval=TRUE}

plot_list <- htmltools::tagList()
seq_count = 4

for (i in seq(from=1,to=length(features.category),by=1))
{
  p <- plot_ly() %>%
     add_trace(x=anchor[,features.category[i]], type='histogram',alpha=.5, name=features.category[i]) %>%
     layout(xaxis=list(title=features.category[i]))

  plot_list[[i]] <- p
}

lattice_list <- htmltools::tagList()

for (i in seq(from=1,to=length(features.category),by=seq_count))
{
  if (length(features.category) - i < seq_count)
  {
    lattice_list[[i]] <- subplot(plot_list[i:length(features.category)],nrows=2)
  } else
  {
    lattice_list[[i]] <- subplot(plot_list[i:(i+seq_count-1)],nrows=2)
  }
}

lattice_list

```
# Boosting

## Extreme Gradient Boosting (XGBoost)

XGBoost is a supervised machine learning technique, where the output is provided.  The **regularization** term is what controls the complexity of the model to prevent overfitting.

In Machine Learning, we talk of the relationship between simplicity and predictability of the model.  The tradeoff is referred to the **bias-variance** tradeoff.

XGBoost is a tree ensemble, which is a set of classification and regression trees (CART).  CART differs from decision trees in that decision trees only provide the decision values, whereas CART provides an associated score with each leaf.

Random forests are also ensemble trees.  The only difference between Random Forests and XG Boost is how the model is trained.  To train the model, XG Boost uses an additive algorithm.  Fix what has been learned and add 1 new tree at a time.

Advantages of XGBoost:

1.  Parallel Computing:  Parallel processing is enabled.  By default, it will use all cores on your machine.
2.  Regularization:  Advtange over GBM package, which does not have regularization.  See Appendix B.
3.  Cross Validation:  Though there are packages out there to handle cross validation, xgboost has an internal CV function.
4.  Missing Values:  XGBoost handles missing values internally.  If missing values have a pattern, it will be captured by the model.
5.  Flexibility:  In addition to regression, classification and ranking problems, you can define custom objective functions.  It also supports user defined evaluation metrics.  
6.  Availability:  It's available in R, Python, Java, Julia and Scala.
7.  Save and Reload:  You can save the model and reload it.  
8.  Tree Pruning:  XGBoost continues to grow a tree until the max depth, and then prunes backwards until the improvement on the loss function is below threshold.

For many machine learning algorithms, having highly correlated variables is not a good idea.  It can make predictions less accurate and makes interpreting the model almost impossible.  GLM's for example assume that features are uncorrelated.

Decision tree algorithms, including boosted trees, are very robust to correlated features and one does not have to do anything to manage the correlations.

For Classification problems, trees are grown to reduce misclassification rates in subsequent iterations.  

For Regression problems, it builds a generalized linear model and optimizes it using regularization (L1, L2) and gradient descent.  Subsequent models are built on residuals (Actual - Predicted).  *Gradient Descent* is a method where a vector of weights (or coefficients), we want to minimize the partial derivatives to find the local minima of the RSS.  Simply put, we want to minimize the error by tuning the values of the coefficients.

For regularization, for linear regression we want to minimize the RSS (objective function) and we apply a penalty term to the objective function.  

Optimizing lambda defines the trade-off between the prediction accuracy of the training sample and prediction accuracy of the hold out sample.

## Utilizing the xgboost package

Description of the parameters for the xgboost package:

### General Parameters
* **booster = "gblinear" or "gbtree":**  allows you to use a linear or classification boosting algorithm
* **nthread = n:**  number of cpu threads to use.  By default it will use the max.
* **silent = 0 or 1:**  1 prints running messages to the console.  Best to keep at 0.

### Parameters for Tree Booster
* **nround = n:** number of passes to make on the data.  Each round will enhance the model by further reducing the difference between the ground truth and prediction.  The more complex the relationship between your features and your label (response variable), the more rounds you will need.  By default it will run 100.
* **eta = 0 - 1:**  By default, it is set to 0.3.  This controls the learning rate, i.e. the rate in which the model learns patterns in data.  After every round, it shrinks the feature weights to reach optimum.  Lower the eta, the slower the computation and must be supported by **nround**.
* **gamma = 0 - inf:**  Controls the regularization.  Higher the value, the higher the regularization.  Default is set to 0, which means no regularization.  As a trick, you can set the gamma to 0 and check the CV error rate.  If the train error >>> test error, bring the gamma into action.  Higher the gamma, the lower the difference between Train and Test CV.  Gamma brings improvement when you want to use shallow trees.
* **max.depth = n:** you can set the max depth of trees.  Larger the depth, higher the chances of overfitting.  This should be tuned with CV.
* **min_child_weight = 0 - inf:**  In Regression, refers to the min number of instances required in a child node.  In Classfication, if the leaf node has a minimum sum of instance weights lower than the min_child_weight, the tree splitting stops.  Basically another feature to prevent overfitting.
* **subsample = 0 - 1:**  controls the number of samples (observations) supplied to a tree.  Typically, this is (0.5,0.8).  Default is 1.
* **colsample_bytree = 0 - 1:** controls the number of features supplied to a tree.  Typically, that is (0.5,0.9).  Default is 1.
* **lambda = 0 or 1:** enables the L2 regularization (Ridge Regression) on weights.  Default is 0
* **alpha = 0 or 1:** enables the L1 regularization (Lasso Regression) on weights.  In addition to shrinkage, alpha also results in feature selection.

### Parameters for Linear Booster
Uses significantly less parameters than the Tree Booster

* **nround = n:** 
* **lambda = 0 or 1:** 
* **alpha = 0 or 1:** 

### Learning Task Parameters

* **objective = (see below):** You can set the objective function.  Here as an example, you can set it as a binary classification model.
  * reg:linear - linear regression
  * binary:logistic - logistic regression for binary classification.  Returns class probabilities
  * multi:softmax - multiclassification using softmax objective.  Returns predicted class labels.  Requires setting num_class parameter denoting number of unique prediction classes.
  * multi:softprob - multiclassification using softmax objective returning predicted class probabilities.
* **eval.metric = (see below):**  metrics used to evaluate model accuracy.  Regression default is RMSE.  Classification default is error.
  * mae - Mean Absolute Error (used in Regression)
  * LogLoss - Negative loglikelihood (used in Classification)
  * AUC - Area under curve (used in Classification)
  * RMSE - Root Mean Square Error (used in Regression)
  * error - Binary classification error rate [# wrong / # total]
  * mlogloss - multiclass logloss (used in Classification)

### Other Parameters
* **verbose = 0,1,2:**  0 is no messages, 1 prints evaluation metric, 2 also prints the information about the tree
* **watchlist = list(train=train,test=test):** allows you to feed a test set into the model so that the model can keep retesting itself on the test set.  This way you can measure the progress of learning.  This will help prevent overfitting and can define a stopping point when the model is optimized and no longer needs any more iterations.

XGBoost produces a regression model.  In a binary classification model, the predictions will be probabilities that range from 0 < p < 1.  To interpret the prediction for the binary classification, you can set the probability to > .5, etc.

For a multiclass classification, the similar concept of assigning a threshold applies.

## Data Preparation for xgboost

xgboost only takes numeric variables.  We will need to convert categorical variables, like state, into a numeric variable that can be fed into xgboost.  We will convert a dense matrix (few zeroes in the matrix) to a sparse matrix (many zeroes in the matrix).

One method is called **one-hot encoding**. It is a respresentation of a categorical variable as a binary vector.

Let's show you an example.  Suppose you have the following sequence:
  'red','red','green'
  
Represented as an integer:

  0,0,1
  
One Hot Encoding Representation would look like:

  [0,1]
  
  [0,1]
  
  [1,0]

But one-hot encoding also presents two problems that are more particular to tree-based models:

* The resulting sparsity virtually ensures that continuous variables are assigned higher feature importance.

* A single level of a categorical variable must meet a very high bar in order to be selected for splitting early in the tree building. This can degrade predictive performance.

*Note:  When possible, it is ideal to use the data.table instead of the data.frame.  data.table is 100% compliant with data.frames and its performance is much better.*

*Note: You will want to either remove the ID field or NULL it out.*

# One Hot Encoding

Now that the variables are separated between Numerical and Categorical, we can One-Hot encode the categorical variables.

```{r One_Hot_Manually, eval=FALSE}
# First one-hot encode the categorical variables
anchor.train.encoded <- anchor.train[,features.category] %>%
  gather(variable, value, -(Id)) %>%
  unite(comb_header, variable, value) %>%
  mutate(value=1) %>%
  dcast(Id~comb_header)

# If not 1, then it is definitely 0
anchor.train.encoded[is.na(anchor.train.encoded)] <- 0
anchor.train.encoded <- left_join(anchor.train.encoded, anchor.train[features.numeric], by="Id") %>% select(-Id)

#anchor.train.encoded <- subset(anchor.train.encoded, select=-c(Id))

anchor.train.encoded <- cbind(anchor.train.encoded, SalePrice=response.train)

colnames(anchor.train.encoded) <- gsub("[^[:alnum:]]", "_",colnames(anchor.train.encoded))

for(i in 1:ncol(anchor.train.encoded)){
  anchor.train.encoded[is.na(anchor.train.encoded[,i]), i] <- mean(anchor.train.encoded[,i], na.rm = TRUE)
}

# Prep the Test data the same way
anchor.test.encoded <- anchor.test[,features.category] %>%
  gather(variable, value, -(Id)) %>%
  unite(comb_header, variable, value) %>%
  mutate(value=1) %>%
  dcast(Id~comb_header)

# If not 1, then it is definitely 0
anchor.test.encoded[is.na(anchor.test.encoded)] <- 0

anchor.test.encoded <- left_join(anchor.test.encoded, anchor.test[features.numeric], by="Id") %>% select(-Id)

colnames(anchor.test.encoded) <- gsub("[^[:alnum:]]", "_",colnames(anchor.test.encoded))

for(i in 1:ncol(anchor.test.encoded)){
  anchor.test.encoded[is.na(anchor.test.encoded[,i]), i] <- mean(anchor.test.encoded[,i], na.rm = TRUE)
}

anchor.test.encoded$SalePrice <- 0

```

```{r Caret_One_Hot, eval=TRUE}

anchor <- anchor %>% dplyr::select(-Id)

colnames(anchor)

dummies <- dummyVars(~.,anchor[features.category])
anchor.encoded <- predict(dummies,anchor[features.category])
colnames(anchor.encoded) <- gsub("[^[:alnum:]]", "_",colnames(anchor.encoded))

anchor.encoded <- cbind(anchor.encoded,anchor[features.numeric])

anchor.train.encoded <- anchor.encoded[1:1456,]
anchor.test.encoded <- anchor.encoded[1457:2915,]

anchor.train.encoded <- cbind(anchor.train.encoded,SalePrice=response.train)
anchor.test.encoded$SalePrice <- 0
```

```{r Divide_Train, eval=TRUE}

intrain <- createDataPartition(y=anchor.train.encoded$SalePrice, p = .7, list=FALSE)

part.train <- anchor.train.encoded[intrain,]
part.validate <- anchor.train.encoded[-intrain,]
```


> Predictions from the MLR (and other xgboost packages) cannot make predictions when the columns are not consistent.  i.e. Train and Test must have the same columns used between both datasets.  If not, the output basically defaults to the average for all rows.  Even though it will make accurate predictions on the training set if you feed it back into the model for predictions.

# Error/Loss/Cost Functions

There are times when appropriate Cost/Loss functions are not available in the `xgb` & `mlr` package to evalute the errors/residuals for machine learning.  Fortunately, you can create your own custom error metric to evaluate the residuals.

As discussed in the outlier analysis, the cost function/residuals are important because gradient boosting buildings upon residuals.  

Absolute and Huber Error metrics are supposed to be more robust to outliers than RMSE.  Seeing how 

**Absolute Error**

```{r Cost_Functions, eval=TRUE, message=FALSE, warning=FALSE}
abse <- function(pred, dtrain){
  labels <- getinfo(dtrain,"label")
  err <- sum(abs(as.numeric(labels) - as.numeric(pred)))
  return(list(metric="abse",value=err))
}
```


**Huber Loss Function**

https://en.wikipedia.org/wiki/Huber_loss

```{r Cost_Functions_Huber, eval=TRUE, message=FALSE, warning=FALSE}
huber_loss <- function(x,y,alpha)
{
  if(abs(x-y) < alpha){
    err <- .5*(x-y)**2
  } else
  {
    err <- alpha*abs(x-y)-.5*alpha**2
  }
  return(err)
}

huber <- function(pred, dtrain, alpha){
  alpha <- ifelse(missing(alpha),.5,alpha)
  labels <- getinfo(dtrain,"label")
  err <- sum(mapply(huber_loss, as.numeric(pred), as.numeric(labels), alpha))
  return(list(metric="huber",value=err))
}

```

```{r XGBoost_Default, eval=FALSE}
dtrain <- xgb.DMatrix(as.matrix(part.train %>% dplyr::select(-SalePrice)),label=as.matrix(part.train$SalePrice))
dtest <- xgb.DMatrix(as.matrix(part.validate %>% dplyr::select(-SalePrice)),label=as.matrix(part.validate$SalePrice))

#xgboost parameters
xgb_params = list(
  booster="gbtree",
  objective = 'reg:linear',
  colsample_bytree = 0.4603,
  subsample = 0.5213,
  eta = 0.01, 
  max_depth = 6,
  #alpha = .4640,
  #lambda = 0.0468,
  gamma = 5,
  min_child_weight = 1.7817
  #base_score = 7.76
)

model.xgcv <- xgb.cv(params=xgb_params
                     ,data=dtrain
                     ,nrounds=3000
                     ,nfold=5  #Cross Validation folds
                     ,showsd = T # Show SD of cross validation
                     ,stratified = T # sample folds should be stratified by the values of outcome labels
                     ,print_every_n = 20
                     ,early_stopping_rounds = 20 # Stops if the performance doesn't improve per K rounds
                     ,maximize=F  
                     #,metrics="mae"
)

best_n_rounds = 824

model.xgboost = xgb.train(xgb_params
                          ,data=dtrain
                          ,nfolds = 5  #Cross Validation
                          #,nthread = 2
                          ,showsd=T
                          #,max.depth = 10
                          ,nrounds = as.integer(best_n_rounds)
                          #,verbose = 1
                          ,watchlist=list(train=dtrain,test=dtest)
                          #,eval.metric = "logloss"
                          )

# Save the predictions on Test data
prediction <- predict(model.xgboost,dtest)
#prediction <- predict(model.xgboost,dtrain)
importance_matrix <- xgb.importance(feature_names=names(dtrain), model=model.xgboost)
xgb.plot.importance(importance_matrix=importance_matrix)

#write.csv(prediction,  paste0(dir.data,"default_2.csv"))
#write.csv(part.train$SalePrice,paste0(dir.data,"temp.csv"))
```

# Machine Learning with MLR

https://www.kaggle.com/aniruddhachakraborty/lasso-gbm-xgboost-top-20-0-12039-using-r

```{r}

mlrn.traintask <- makeRegrTask(data=anchor.train.encoded,target="SalePrice")
#mlrn.traintask <- makeRegrTask(data=anchor.train.encoded.rmsle,target="SalePrice")
#mlrn.traintask <- createDummyFeatures(mlrn.traintask)

mlrn.lrn <- makeLearner("regr.xgboost")

mlrn.lrn$par.vals = list(
  #nthread = 8   #Commented Out so that it uses the max
  nrounds = 100
  #,alpha = 1
  #,lambda=1
  ,print_every_n = 10
  ,objective = "reg:linear"
  ,early_stopping_rounds = 10  # Helps to optimize efficiency since we have high number of rounds
  ,eval.metric = "rmse"
  #,feval=huber
  ,maximize=F #Needed with custom loss function
)

mlrn.params <- makeParamSet( 
  makeDiscreteParam("booster",values = c("gbtree"))
  ,makeIntegerParam("gamma",lower=0,upper=10)
  #,makeNumericParam("lambda",lower=1,upper=5)
  ,makeIntegerParam("max_depth",lower = 3,upper = 25)
  ,makeNumericParam("eta",lower=0.005,upper=0.3)  # Should be tuned with n rounds
  ,makeNumericParam("min_child_weight",lower = 1,upper = 20)
  ,makeNumericParam("subsample",lower = 0.3,upper = 1)
  ,makeNumericParam("colsample_bytree",lower = 0.2,upper = 1)
)

mlrn.rdesc <- makeResampleDesc("CV",iters=5)  # Performs n-fold cross validation
mlrn.ctrl <- makeTuneControlRandom(maxit = 5)  # Optimization Algorithm

```

```{r}
Sys.time()
mytune <- tuneParams(learner = mlrn.lrn
                     ,task = mlrn.traintask
                     ,resampling = mlrn.rdesc
                     ,par.set = mlrn.params
                     ,control = mlrn.ctrl
                     ,measures=rmse
)
Sys.time()
```

```{r}
#colnames(anchor.test.encoded)
listMeasures(mlrn.traintask)

#blah2 <- subset(anchor.test.encoded,select=c(GrLivArea,TotalBsmtSF,OverallQual,response_train))
#mlrn.testtask <- makeRegrTask(data=anchor.test.encoded,target="response_train")
mlrn.testtask <- makeRegrTask(data=anchor.test.encoded,target="SalePrice")
#mlrn.testtask <- createDummyFeatures(mlrn.testtask)
```


```{r}
# Just shows you the parameters selected
mytune$x
mytune$y

# Retrieves the tuned parameters
mlrn.tuned_param <- setHyperPars(learner=mlrn.lrn
                                 ,par.vals=mytune$x)

#resample(mlrn.tuned_param, mlrn.traintask,mlrn.rdesc,measures=list(rmse,mse))

# I want to see the hyper tuning effects
hypdata <- generateHyperParsEffectData(mytune, partial.dep = TRUE)

plotHyperParsEffect(hypdata, x = "iteration", y = "rmse.test.rmse", plot.type = "line",
  partial.dep.learn = "regr.randomForest")

#plotHyperParsEffect(hypdata, x = "iteration", y = "rmsle.test.mean", plot.type = "line",
#  partial.dep.learn = "regr.randomForest")

plot_ly(type="scatter",mode="lines+markers"
        ,x=hypdata$data$iteration
        ,y=hypdata$data$rmse.test.rmse)

#plot_ly(type="scatter",mode="lines+markers"
#        ,x=hypdata$data$iteration
#        ,y=hypdata$data$rmsle.test.mean)


```

```{r}
model <- mlr::train(mlrn.tuned_param, mlrn.traintask)

model$learner$par.vals

prediction.tuned <- predict(model, mlrn.testtask)
#prediction <- predict(model, mlrn.traintask)

prediction.tuned
prediction

write.csv(prediction,  paste0(dir.data,"train_results_8.csv"))
write.csv(prediction.tuned,  paste0(dir.data,"final_13.csv"))
```


```{r}
importance_matrix_tuned <- getFeatureImportance(model)
xgb_linear <- data.frame(Feature=names(importance_matrix_tuned$res),Importance=t(importance_matrix_tuned$res))
xgb_linear$Feature <- factor(xgb_linear$Feature,levels=unique(xgb_linear$Feature)[order(xgb_linear$Importance,decreasing=FALSE)])
xgb_linear <- xgb_linear %>% arrange(desc(Importance))

xgb_linear

#write.csv(xgb_linear,paste0(dir.data,"important.csv"))
```


If the datasets are large and you don't want to spend the time having to retrain the model in the future.  You can save the model.
```{r, eval=FALSE}
#To Save
xgb.save(model.xgboost,"model.xgboost")

#To Load a Saved Model
model.xgboost2 <-  xgb.load("model.xgboost")
```

# Lasso Regression with Caret

```{r}

ct.control <- trainControl(method="cv",number=10)
lassoGrid <- expand.grid(alpha=1,lambda=seq(.001,.1,by=.0005))

lasso.data <- anchor.train %>% dplyr::select(-SalePrice)

lasso_mod <- train(x=anchor.train.encoded, y=response.train, method="glmnet",trControl=ct.control, tuneGrid = lassoGrid)

lasso_mod$bestTune
 
min(lasso_mod$results$RMSE)

varImp(lasso_mod, scale=F)

predictions.lasso <- predict(lasso_mod, anchor.train.encoded)

head(predictions.lasso)

#write.csv(predictions.lasso, paste0(dir.data,"lasso_1.csv"))

```

# Results

## Comparing Importance Results

```{r Comparison, eval=TRUE}
xgb_df <- data.frame(importance_matrix)
xgb_df$Feature <- factor(xgb_df$Feature,levels=unique(xgb_df$Feature)[order(xgb_df$Gain,decreasing=FALSE)])

nfeats = 15

secondaxis <- list(overlaying="y",side="right",title="XGBoost")

p_boruta <- plot_ly(x=head(results_df$meanImp,n=nfeats)
                    ,y=head(results_df$feature,n=nfeats)
                    ,type="bar"
                    ,orientation = "h"
                    ,name="Boruta")
p_xgb <- plot_ly(x=head(xgb_df$Gain,n=nfeats)
                 ,y=head(xgb_df$Feature,n=nfeats)
                 ,type="bar",orientation="h", name="XGBoost") %>% 
  layout(title="Boruta vs XG Boost - Feature Importance")

p_compare <- subplot(p_boruta,p_xgb)

#htmlwidgets::saveWidget(p_compare, "Boruta vs XGBoost Feature Importance.html")

fluidPage(
  fluidRow(p_compare)
)


```

#Appendix A. Data Dictionary

MSSubClass: Identifies the type of dwelling involved in the sale.	

        20	1-STORY 1946 & NEWER ALL STYLES
        30	1-STORY 1945 & OLDER
        40	1-STORY W/FINISHED ATTIC ALL AGES
        45	1-1/2 STORY - UNFINISHED ALL AGES
        50	1-1/2 STORY FINISHED ALL AGES
        60	2-STORY 1946 & NEWER
        70	2-STORY 1945 & OLDER
        75	2-1/2 STORY ALL AGES
        80	SPLIT OR MULTI-LEVEL
        85	SPLIT FOYER
        90	DUPLEX - ALL STYLES AND AGES
       120	1-STORY PUD (Planned Unit Development) - 1946 & NEWER
       150	1-1/2 STORY PUD - ALL AGES
       160	2-STORY PUD - 1946 & NEWER
       180	PUD - MULTILEVEL - INCL SPLIT LEV/FOYER
       190	2 FAMILY CONVERSION - ALL STYLES AND AGES

MSZoning: Identifies the general zoning classification of the sale.
		
       A	Agriculture
       C	Commercial
       FV	Floating Village Residential
       I	Industrial
       RH	Residential High Density
       RL	Residential Low Density
       RP	Residential Low Density Park 
       RM	Residential Medium Density
	
LotFrontage: Linear feet of street connected to property

LotArea: Lot size in square feet

Street: Type of road access to property

       Grvl	Gravel	
       Pave	Paved
       	
Alley: Type of alley access to property

       Grvl	Gravel
       Pave	Paved
       NA 	No alley access
		
LotShape: General shape of property

       Reg	Regular	
       IR1	Slightly irregular
       IR2	Moderately Irregular
       IR3	Irregular
       
LandContour: Flatness of the property

       Lvl	Near Flat/Level	
       Bnk	Banked - Quick and significant rise from street grade to building
       HLS	Hillside - Significant slope from side to side
       Low	Depression
		
Utilities: Type of utilities available
		
       AllPub	All public Utilities (E,G,W,& S)	
       NoSewr	Electricity, Gas, and Water (Septic Tank)
       NoSeWa	Electricity and Gas Only
       ELO	Electricity only	
	
LotConfig: Lot configuration

       Inside	Inside lot
       Corner	Corner lot
       CulDSac	Cul-de-sac
       FR2	Frontage on 2 sides of property
       FR3	Frontage on 3 sides of property
	
LandSlope: Slope of property
		
       Gtl	Gentle slope
       Mod	Moderate Slope	
       Sev	Severe Slope
	
Neighborhood: Physical locations within Ames city limits

       Blmngtn	Bloomington Heights
       Blueste	Bluestem
       BrDale	Briardale
       BrkSide	Brookside
       ClearCr	Clear Creek
       CollgCr	College Creek
       Crawfor	Crawford
       Edwards	Edwards
       Gilbert	Gilbert
       IDOTRR	Iowa DOT and Rail Road
       MeadowV	Meadow Village
       Mitchel	Mitchell
       Names	North Ames
       NoRidge	Northridge
       NPkVill	Northpark Villa
       NridgHt	Northridge Heights
       NWAmes	Northwest Ames
       OldTown	Old Town
       SWISU	South & West of Iowa State University
       Sawyer	Sawyer
       SawyerW	Sawyer West
       Somerst	Somerset
       StoneBr	Stone Brook
       Timber	Timberland
       Veenker	Veenker
			
Condition1: Proximity to various conditions
	
       Artery	Adjacent to arterial street
       Feedr	Adjacent to feeder street	
       Norm	Normal	
       RRNn	Within 200' of North-South Railroad
       RRAn	Adjacent to North-South Railroad
       PosN	Near positive off-site feature--park, greenbelt, etc.
       PosA	Adjacent to postive off-site feature
       RRNe	Within 200' of East-West Railroad
       RRAe	Adjacent to East-West Railroad
	
Condition2: Proximity to various conditions (if more than one is present)
		
       Artery	Adjacent to arterial street
       Feedr	Adjacent to feeder street	
       Norm	Normal	
       RRNn	Within 200' of North-South Railroad
       RRAn	Adjacent to North-South Railroad
       PosN	Near positive off-site feature--park, greenbelt, etc.
       PosA	Adjacent to postive off-site feature
       RRNe	Within 200' of East-West Railroad
       RRAe	Adjacent to East-West Railroad
	
BldgType: Type of dwelling
		
       1Fam	Single-family Detached	
       2FmCon	Two-family Conversion; originally built as one-family dwelling
       Duplx	Duplex
       TwnhsE	Townhouse End Unit
       TwnhsI	Townhouse Inside Unit
	
HouseStyle: Style of dwelling
	
       1Story	One story
       1.5Fin	One and one-half story: 2nd level finished
       1.5Unf	One and one-half story: 2nd level unfinished
       2Story	Two story
       2.5Fin	Two and one-half story: 2nd level finished
       2.5Unf	Two and one-half story: 2nd level unfinished
       SFoyer	Split Foyer
       SLvl	Split Level
	
OverallQual: Rates the overall material and finish of the house

       10	Very Excellent
       9	Excellent
       8	Very Good
       7	Good
       6	Above Average
       5	Average
       4	Below Average
       3	Fair
       2	Poor
       1	Very Poor
	
OverallCond: Rates the overall condition of the house

       10	Very Excellent
       9	Excellent
       8	Very Good
       7	Good
       6	Above Average	
       5	Average
       4	Below Average	
       3	Fair
       2	Poor
       1	Very Poor
		
YearBuilt: Original construction date

YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)

RoofStyle: Type of roof

       Flat	Flat
       Gable	Gable
       Gambrel	Gabrel (Barn)
       Hip	Hip
       Mansard	Mansard
       Shed	Shed
		
RoofMatl: Roof material

       ClyTile	Clay or Tile
       CompShg	Standard (Composite) Shingle
       Membran	Membrane
       Metal	Metal
       Roll	Roll
       Tar&Grv	Gravel & Tar
       WdShake	Wood Shakes
       WdShngl	Wood Shingles
		
Exterior1st: Exterior covering on house

       AsbShng	Asbestos Shingles
       AsphShn	Asphalt Shingles
       BrkComm	Brick Common
       BrkFace	Brick Face
       CBlock	Cinder Block
       CemntBd	Cement Board
       HdBoard	Hard Board
       ImStucc	Imitation Stucco
       MetalSd	Metal Siding
       Other	Other
       Plywood	Plywood
       PreCast	PreCast	
       Stone	Stone
       Stucco	Stucco
       VinylSd	Vinyl Siding
       Wd Sdng	Wood Siding
       WdShing	Wood Shingles
	
Exterior2nd: Exterior covering on house (if more than one material)

       AsbShng	Asbestos Shingles
       AsphShn	Asphalt Shingles
       BrkComm	Brick Common
       BrkFace	Brick Face
       CBlock	Cinder Block
       CemntBd	Cement Board
       HdBoard	Hard Board
       ImStucc	Imitation Stucco
       MetalSd	Metal Siding
       Other	Other
       Plywood	Plywood
       PreCast	PreCast
       Stone	Stone
       Stucco	Stucco
       VinylSd	Vinyl Siding
       Wd Sdng	Wood Siding
       WdShing	Wood Shingles
	
MasVnrType: Masonry veneer type

       BrkCmn	Brick Common
       BrkFace	Brick Face
       CBlock	Cinder Block
       None	None
       Stone	Stone
	
MasVnrArea: Masonry veneer area in square feet

ExterQual: Evaluates the quality of the material on the exterior 
		
       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       Po	Poor
		
ExterCond: Evaluates the present condition of the material on the exterior
		
       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       Po	Poor
		
Foundation: Type of foundation
		
       BrkTil	Brick & Tile
       CBlock	Cinder Block
       PConc	Poured Contrete	
       Slab	Slab
       Stone	Stone
       Wood	Wood
		
BsmtQual: Evaluates the height of the basement

       Ex	Excellent (100+ inches)	
       Gd	Good (90-99 inches)
       TA	Typical (80-89 inches)
       Fa	Fair (70-79 inches)
       Po	Poor (<70 inches
       NA	No Basement
		
BsmtCond: Evaluates the general condition of the basement

       Ex	Excellent
       Gd	Good
       TA	Typical - slight dampness allowed
       Fa	Fair - dampness or some cracking or settling
       Po	Poor - Severe cracking, settling, or wetness
       NA	No Basement
	
BsmtExposure: Refers to walkout or garden level walls

       Gd	Good Exposure
       Av	Average Exposure (split levels or foyers typically score average or above)	
       Mn	Mimimum Exposure
       No	No Exposure
       NA	No Basement
	
BsmtFinType1: Rating of basement finished area

       GLQ	Good Living Quarters
       ALQ	Average Living Quarters
       BLQ	Below Average Living Quarters	
       Rec	Average Rec Room
       LwQ	Low Quality
       Unf	Unfinshed
       NA	No Basement
		
BsmtFinSF1: Type 1 finished square feet

BsmtFinType2: Rating of basement finished area (if multiple types)

       GLQ	Good Living Quarters
       ALQ	Average Living Quarters
       BLQ	Below Average Living Quarters	
       Rec	Average Rec Room
       LwQ	Low Quality
       Unf	Unfinshed
       NA	No Basement

BsmtFinSF2: Type 2 finished square feet

BsmtUnfSF: Unfinished square feet of basement area

TotalBsmtSF: Total square feet of basement area

Heating: Type of heating
		
       Floor	Floor Furnace
       GasA	Gas forced warm air furnace
       GasW	Gas hot water or steam heat
       Grav	Gravity furnace	
       OthW	Hot water or steam heat other than gas
       Wall	Wall furnace
		
HeatingQC: Heating quality and condition

       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       Po	Poor
		
CentralAir: Central air conditioning

       N	No
       Y	Yes
		
Electrical: Electrical system

       SBrkr	Standard Circuit Breakers & Romex
       FuseA	Fuse Box over 60 AMP and all Romex wiring (Average)	
       FuseF	60 AMP Fuse Box and mostly Romex wiring (Fair)
       FuseP	60 AMP Fuse Box and mostly knob & tube wiring (poor)
       Mix	Mixed
		
1stFlrSF: First Floor square feet
 
2ndFlrSF: Second floor square feet

LowQualFinSF: Low quality finished square feet (all floors)

GrLivArea: Above grade (ground) living area square feet

BsmtFullBath: Basement full bathrooms

BsmtHalfBath: Basement half bathrooms

FullBath: Full bathrooms above grade

HalfBath: Half baths above grade

Bedroom: Bedrooms above grade (does NOT include basement bedrooms)

Kitchen: Kitchens above grade

KitchenQual: Kitchen quality

       Ex	Excellent
       Gd	Good
       TA	Typical/Average
       Fa	Fair
       Po	Poor
       	
TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)

Functional: Home functionality (Assume typical unless deductions are warranted)

       Typ	Typical Functionality
       Min1	Minor Deductions 1
       Min2	Minor Deductions 2
       Mod	Moderate Deductions
       Maj1	Major Deductions 1
       Maj2	Major Deductions 2
       Sev	Severely Damaged
       Sal	Salvage only
		
Fireplaces: Number of fireplaces

FireplaceQu: Fireplace quality

       Ex	Excellent - Exceptional Masonry Fireplace
       Gd	Good - Masonry Fireplace in main level
       TA	Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement
       Fa	Fair - Prefabricated Fireplace in basement
       Po	Poor - Ben Franklin Stove
       NA	No Fireplace
		
GarageType: Garage location
		
       2Types	More than one type of garage
       Attchd	Attached to home
       Basment	Basement Garage
       BuiltIn	Built-In (Garage part of house - typically has room above garage)
       CarPort	Car Port
       Detchd	Detached from home
       NA	No Garage
		
GarageYrBlt: Year garage was built
		
GarageFinish: Interior finish of the garage

       Fin	Finished
       RFn	Rough Finished	
       Unf	Unfinished
       NA	No Garage
		
GarageCars: Size of garage in car capacity

GarageArea: Size of garage in square feet

GarageQual: Garage quality

       Ex	Excellent
       Gd	Good
       TA	Typical/Average
       Fa	Fair
       Po	Poor
       NA	No Garage
		
GarageCond: Garage condition

       Ex	Excellent
       Gd	Good
       TA	Typical/Average
       Fa	Fair
       Po	Poor
       NA	No Garage
		
PavedDrive: Paved driveway

       Y	Paved 
       P	Partial Pavement
       N	Dirt/Gravel
		
WoodDeckSF: Wood deck area in square feet

OpenPorchSF: Open porch area in square feet

EnclosedPorch: Enclosed porch area in square feet

3SsnPorch: Three season porch area in square feet

ScreenPorch: Screen porch area in square feet

PoolArea: Pool area in square feet

PoolQC: Pool quality
		
       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       NA	No Pool
		
Fence: Fence quality
		
       GdPrv	Good Privacy
       MnPrv	Minimum Privacy
       GdWo	Good Wood
       MnWw	Minimum Wood/Wire
       NA	No Fence
	
MiscFeature: Miscellaneous feature not covered in other categories
		
       Elev	Elevator
       Gar2	2nd Garage (if not described in garage section)
       Othr	Other
       Shed	Shed (over 100 SF)
       TenC	Tennis Court
       NA	None
		
MiscVal: $Value of miscellaneous feature

MoSold: Month Sold (MM)

YrSold: Year Sold (YYYY)

SaleType: Type of sale
		
       WD 	Warranty Deed - Conventional
       CWD	Warranty Deed - Cash
       VWD	Warranty Deed - VA Loan
       New	Home just constructed and sold
       COD	Court Officer Deed/Estate
       Con	Contract 15% Down payment regular terms
       ConLw	Contract Low Down payment and low interest
       ConLI	Contract Low Interest
       ConLD	Contract Low Down
       Oth	Other
		
SaleCondition: Condition of sale

       Normal	Normal Sale
       Abnorml	Abnormal Sale -  trade, foreclosure, short sale
       AdjLand	Adjoining Land Purchase
       Alloca	Allocation - two linked properties with separate deeds, typically condo with a garage unit	
       Family	Sale between family members
       Partial	Home was not completed when last assessed (associated with New Homes)
       
#Appendix B.  XGBoost Resources

**Introduction to Boosted Trees:**

http://xgboost.readthedocs.io/en/latest/model.html

**XGBoost R Tutorial can be found:**

http://xgboost.readthedocs.io/en/latest/R-package/xgboostPresentation.html

**Exploratory Data Analysis with XGBoost:**

http://xgboost.readthedocs.io/en/latest/R-package/discoverYourData.html

**Beginner's Tutorial to XGBoost:**

https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/beginners-tutorial-on-xgboost-parameter-tuning-r/tutorial/

**Full documentation on Parameters:**

http://xgboost.readthedocs.io/en/latest//parameter.html

**Complete Tutorial on Tree Based Modeling:**

https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/
       
#Appendix C.  Preventing Overfitting

There are 4 commonly used methods to prevent overfitting:

1.  Cross-Validation:  Leave one sample out for in-time validation and the rest of the data used for training.

2.  Early Stopping:  Provides guidance on how many iterations before the learner begins to overfit.

3.  Pruning:  Removing nodes which add little predictive power to the model.

4.  Regularization:  Introduces a cost term for bringing more features with the objective function.  

#Appendix D.  Imputation

## Background

It seems that the general consensus is that there is no good way to handle missing values.  There may be instances where leaving the data row out when it has values missing will be a good choice, however, you might be still depriving the dataset of valuable information.  On the flip side, depending on how you impute the missing data, you may be introducing bias into the data.  

In general, missing values can be bucketed into 3 categories:

  1.  **Missing at Random (MAR):**  Data points that are missing not related to the missing data, but related to some of the observed data.
  2.  **Missing Completely at Random (MCAR):**  Missing data has nothing to do with the hypothesis or values of the other variables.
  3.  **Missing not at Random (MNAR):**  Missing value is dependent on some hypothetical value or dependent on some other variable.

In cases 1 & 2, it is safe to delete the data with the missing values, dependent on the occurrences.  However,in case 3, removing the data can introduce bias into the model.

>*Note: Imputation does not necessarily mean better results*

## Imputing Missing Data: Categorical vs Numerical

Categorical:
  1.  Mode Imputation, but it will definitely introduce bias
  2.  Missing values can be treated as a separate category, which is the simplest method
  3.  Prediction models.  Here you divide the data into 2 sets: one with no missing values (train) and the data with missing values (test).  Then use logistic regression or ANOVA for prediction.
  4.  Multiple Imputation

Numerical:
  1.  Mean, Mode, Median: Disadvantage is that it reduces the variance of the data
  2.  Linear Regression:  Use the complete cases to build a regression model to predict the missing values.  The disadvantages is that the predicted values fit together "too well" and standard error is deflated.  One is also assuming there is a linear relationship between variables, which there may not be one.
  3.  KNN:  It's simple to understand and easy to implement.  The non-parametric nature of KNN gives it an advantage where data may be highly "unusual".  Disadvantage is that analyzing large data sets can be time consuming.  Also, KNN be severly degraded with high dimensionality as there is little difference between the nearest and farthest neighbors.
  4.  Multiple Imputation:  More detail below

## Multiple Imputation

Multiple imputation is a statistical technique for analyzing incomplete data sets, that is, data sets for which some entries are missing. Application of the technique requires three steps: imputation, analysis and pooling. The figure illustrates these steps.

  1.  **Imputation:** Impute (=fill in) the missing entries of the incomplete data sets, not once, but m times (m=3 in the figure). Imputed values are drawn for a distribution (that can be different for each missing entry). This step results is m complete data sets. 
  2.  **Analysis:** Analyze each of the m completed data sets. This step results in m analyses.
  3.  **Pooling:** Integrate the m analysis results into a final result. Simple rules exist for combining the m analyses.

There are numerous ways to do the multiple imputation.  From the `mice` package, we have the following methods.  

```{r}
methods(mice)
```


We are starting with the **Predictive Mean Matching** method.  This is a semi-parametric appraoch that fills in the missing values by randomly selecting from the observed donor values from an observation whose regression-predicted values are closest to the regression-predicted value for the missing value from the simulated regression model.  This ensures that the imputed values are plausible.


```{r, eval=FALSE}

imp.train <- mice(anchor[c("LotFrontage","MasVnrArea","GarageYrBlt")]
     ,method="pmm"
     ,m=5
     ,maxit=50
     ,seed=1212)

imp.test <- mice(anchor.test[c("LotFrontage","MasVnrArea","GarageYrBlt")]
     ,method="pmm"
     ,m=5
     ,maxit=50
     ,seed=1587)

```

```{r, eval=FALSE}

imp.train$imp$LotFrontage

imp.train$imp$LotFrontage

imp$imp$LotFrontage

imp$imp$GarageYrBlt
```

```{r, eval=FALSE}
imp.train.completed <- complete(imp.train)
imp.test.completed <- complete(imp.test)
```

Replace the original columns with the imputed ones and check to see if there are anymore NA's.

```{r, eval=FALSE}

anchor <- anchor %>% dplyr::select(-LotFrontage, -GarageYrBlt, -MasVnrArea)
anchor <- cbind(anchor, imp.train.completed)

anchor.test <- anchor.test %>% dplyr::select(-LotFrontage, -GarageYrBlt, -MasVnrArea)
anchor.test <- cbind(anchor.test, imp.test.completed)

colnames(anchor.test)
sapply(anchor, function(x) sum(is.na(x)))
```

